{
  "$id": "$gai-technique/indirect_prompt_injection",
  "$schema": "../schema/technique.schema.json",
  "$type": "technique",
  "description": "An adversary may inject prompts indirectly via separate data channel ingested by the LLM such as include text or multimedia pulled from databases or websites. These malicious prompts may be hidden or obfuscated from the user. This type of injection may be used by the adversary to gain a foothold in the system or to target an unwitting user of the system.",
  "external_references": [],
  "framework_references": [
    {
      "framework_id": "AML.T0051.000",
      "framework_name": "MITRE ATLAS",
      "href": "https://atlas.mitre.org/techniques/AML.T0051.000"
    }
  ],
  "name": "Indirect Prompt Injection",
  "object_references": [
    {
      "$id": "$gai-technique/llm_prompt_injection",
      "$type": "technique",
      "description": "Sub-technique of",
      "is_sub_object": true
    }
  ]
}
