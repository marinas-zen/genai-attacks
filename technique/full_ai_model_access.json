{
  "$id": "$gai-technique/full_ai_model_access",
  "$schema": "../schema/technique.schema.json",
  "$type": "technique",
  "description": "Adversaries may gain full \"white-box\" access to an AI model. This means the adversary has complete knowledge of the model architecture, its parameters, and class ontology. They may exfiltrate the model to craft adversarial data and verify attacks offline where it is hard to detect their behavior.",
  "external_references": [],
  "framework_references": [
    {
      "framework_id": "AML.T0044",
      "framework_name": "MITRE ATLAS",
      "href": "https://atlas.mitre.org/techniques/AML.T0044"
    }
  ],
  "name": "Full AI Model Access",
  "object_references": [
    {
      "$id": "$gai-tactic/ai_model_access",
      "$type": "tactic",
      "description": "Obtaining full access to AI models, allowing an adversary to inspect, manipulate, or exfiltrate model data and configurations."
    }
  ]
}
