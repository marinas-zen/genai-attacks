{
  "$id": "$gai-technique/credentials_from_ai_agent_configuration",
  "$schema": "../schema/technique.schema.json",
  "$type": "technique",
  "description": "Adversaries may access the credentials of other tools or services on a system from the configuration of an AI agent. AI agents often utilize external tools or services to take actions.\n\nAI Agents often utilize external tools or services to take actions, such as querying databases, invoking APIs, or interacting with cloud resources. To enable these functions, credentials like API keys, tokens, and connection strings are frequently stored in configuration files. While there are secure methods such as dedicated secret managers or encrypted vaults that can be deployed to store and manage these credentials, in practice they are often placed in less protected locations for convenience or ease of deployment. If an attacker can read or extract these configurations, they may obtain valid credentials that allow direct access to sensitive systems outside the agent itself.",
  "external_references": [],
  "framework_references": [
    {
      "framework_id": "AML.T0083",
      "framework_name": "MITRE ATLAS",
      "href": "https://atlas.mitre.org/techniques/AML.T0083"
    }
  ],
  "name": "Credentials from AI Agent Configuration",
  "object_references": [
    {
      "$id": "$gai-tactic/credential_access",
      "$type": "tactic",
      "description": "Extracting hardcoded or misconfigured credentials from AI agent settings or configuration files."
    }
  ]
}
