{
  "$id": "$gai-technique/llm_prompt_injection",
  "$schema": "../schema/technique.schema.json",
  "$type": "technique",
  "description": "An adversary may craft malicious prompts as inputs to an LLM that cause the LLM to act in unintended ways. These \"prompt injections\" are often designed to cause the model to ignore aspects of its original instructions and follow the adversary's instructions instead.\n\nPrompt Injections can be an initial access vector to the LLM that provides the adversary with a foothold to carry out other steps in their operation. They may be designed to bypass defenses in the LLM, or allow the adversary to issue privileged commands. The effects of a prompt injection can persist throughout an interactive session with an LLM.\n\nMalicious prompts may be injected directly by the adversary (Direct) either to leverage the LLM to generate harmful content or to gain a foothold on the system and lead to further effects. Prompts may also be injected indirectly when as part of its normal operation the LLM ingests the malicious prompt from another data source (Indirect). This type of injection can be used by the adversary to a foothold on the system or to target the user of the LLM. Malicious prompts may also be Triggered by user inputs defined by the adversary.",
  "external_references": [
    {
      "href": "https://simonwillison.net/2022/Sep/12/prompt-injection/",
      "source": "Simon Willison's blog",
      "title": "Prompt injection attacks against GPT-3."
    },
    {
      "href": "https://embracethered.com/blog/posts/2023/ai-injections-direct-and-indirect-prompt-injection-basics/",
      "source": "Embrace the Red",
      "title": "AI Injections: Direct and Indirect Prompt Injections and Their Implications."
    }
  ],
  "framework_references": [
    {
      "framework_id": "AML.T0051",
      "framework_name": "MITRE ATLAS",
      "href": "https://atlas.mitre.org/techniques/AML.T0051"
    }
  ],
  "name": "LLM Prompt Injection",
  "object_references": [
    {
      "$id": "$gai-tactic/execution",
      "$type": "tactic",
      "description": "An adversary can change the execution flow of a GenAI app by controlling a part of its data."
    },
    {
      "$id": "$gai-entity/jonathan_cefalu",
      "$type": "entity",
      "description": "The concept of prompt injection was first discovered by Jonathan Cefalu from Preamble in May 2022 in a letter to OpenAI who called it \"command injection\"."
    },
    {
      "$id": "$gai-mitigation/information_flow_control",
      "$type": "mitigation",
      "description": "Information Flow control would be one of many ways to track and control indirect prompt injections from knowledge corruption and leaking sensitive data."
    },
    {
      "$id": "$gai-mitigation/spotlighting",
      "$type": "mitigation",
      "description": "By spotlighting in prompts, the LLM focuses on a specific part of the query that defines the task, thus avoiding other injected tasks."
    },
    {
      "$id": "$gai-mitigation/llm_activations",
      "$type": "mitigation",
      "description": "By tracking LLM activations, the LLMs shift of attention to different tasks caused by indirect prompt injections can be tracked and mitigated."
    },
    {
      "$id": "$gai-entity/simon_willison",
      "$type": "entity",
      "description": "Simon has coined the term Prompt Injection in his post \"Prompt injection attacks against GPT-3.\""
    }
  ]
}
